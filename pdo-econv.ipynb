{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  对于PDO_eConvs的简易复现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据集 Rotated Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rotated_mnist(batch_size):\n",
    "    #导入数据，将.amat格式转换为numpy array\n",
    "    data_train = np.loadtxt('C:\\\\Users\\\\roderickzzc\\\\Desktop\\\\project\\\\pdo-ecov\\\\mnist_rotation_new\\\\mnist_all_rotation_normalized_float_train_valid.amat')\n",
    "    data_test = np.loadtxt('C:\\\\Users\\\\roderickzzc\\\\Desktop\\\\project\\\\pdo-ecov\\\\mnist_rotation_new\\\\mnist_all_rotation_normalized_float_test.amat')\n",
    "\n",
    "    # get train image datas\n",
    "    x_train_val = data_train[:, :-1] / 1.0\n",
    "    #由于原始数据集默认为784*1，现改为28*28\n",
    "    x_train_val=np.reshape(x_train_val,(12000,1,28,28))\n",
    "    x_test = data_test[:, :-1] / 1.0\n",
    "    x_test=np.reshape(x_test,(50000,1,28,28))\n",
    "    # get train image labels\n",
    "    y_train_val = data_train[:, -1:]\n",
    "    y_test = data_test[:, -1:]\n",
    "    print(x_train_val[0].shape)\n",
    "    \n",
    "    # pytorch data loader\n",
    "    #根据论文抽取2000个样本from training set作为validation\n",
    "    train_val = torch.utils.data.TensorDataset(torch.Tensor(x_train_val), torch.Tensor(y_train_val))\n",
    "    train, val = torch.utils.data.random_split(train_val, [10000,2000])\n",
    "    test = torch.utils.data.TensorDataset(torch.Tensor(x_test), torch.Tensor(y_test))\n",
    "    ## feature, label = train[0]\n",
    "    ## print(feature.shape, label) \n",
    "    train_iter = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "\n",
    "\n",
    "    return train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "train_iter, val_iter, test_iter = load_rotated_mnist(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a subgroup of O(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义所有的differential operator的filter\n",
    "#这里是未经*1/h,1/(h^2),1/(h^3),1/(h^4)的版本\n",
    "from sympy import *\n",
    "d_0=torch.tensor([[0,0,0,0,0], [0,0,0,0,0], [0,0,1,0,0],[0,0,0,0,0],[0,0,0,0,0]])\n",
    "d_x=Matrix([[0,0,0,0,0], [0,0,0,0,0], [0,-1/2,0,1/2,0],[0,0,0,0,0],[0,0,0,0,0]])\n",
    "d_y=Matrix([ [0,0,0,0,0],[0,0,1/2,0,0], [0,0,0,0,0],[0,0,-1/2,0,0],[0,0,0,0,0]])\n",
    "d_xx=Matrix([ [0,0,0,0,0],[0,0,0,0,0], [0,1,-2,1,0],[0,0,0,0,0],[0,0,0,0,0]])\n",
    "d_xy=Matrix([ [0,0,0,0,0],[0,-1/4,0,1/4,0], [0,0,0,0,0],[0,1/4,0,-1/4,0],[0,0,0,0,0]])\n",
    "d_yy=Matrix([ [0,0,0,0,0],[0,0,1,0,0], [0,0,-2,0,0],[0,0,1,0,0],[0,0,0,0,0]])\n",
    "d_xxy=Matrix([[0,0,0,0,0],[0,1/2,-1,1/2,0], [0,0,0,0,0],[0,-1/2,1,-1/2,0],[0,0,0,0,0]])\n",
    "d_xyy=Matrix([[0,0,0,0,0],[0,-1/2,0,1/2,0], [0,1,0,-1,0],[0,-1/2,0,1/2,0],[0,0,0,0,0]])\n",
    "d_xxyy=Matrix([[0,0,0,0,0],[0,1,-2,1,0], [0,-2,4,-2,0],[0,1,-2,1,0],[0,0,0,0,0]])\n",
    "d_xxx=Matrix([[0,0,0,0,0],[0,0,0,0,0], [-1/2,1,0,-1,1/2],[0,0,0,0,0],[0,0,0,0,0]])\n",
    "d_yyy=Matrix([[0,0,1/2,0,0],[0,0,-1,0,0], [0,0,0,0,0],[0,0,1,0,0],[0,0,-1/2,0,0]])\n",
    "d_xxxx=Matrix([[0,0,0,0,0],[0,0,0,0,0], [1,-4,6,-4,1],[0,0,0,0,0],[0,0,0,0,0]])\n",
    "d_xxxy=Matrix([[0,0,0,0,0],[-1/4,1/2,0,-1/2,1/4], [0,0,0,0,0],[1/4,-1/2,0,1/2,-1/4],[0,0,0,0,0]])\n",
    "d_xyyy=Matrix([[0,-1/4,0,1/4,0],[0,1/2,0,-1/2,0], [0,0,0,0,0],[0,-1/2,0,1/2,0],[0,1/4,0,-1/4,0]])\n",
    "d_yyyy=Matrix([[0,0,1,0,0],[0,0,-4,0,0], [0,0,6,0,0],[0,0,-4,0,0],[0,0,1,0,0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义将pdo代入polynomail的函数\n",
    "u,v=symbols(\"u v\")\n",
    "def subs_pdo(P, *args):\n",
    "\n",
    "    P=expand(P)\n",
    "  \n",
    "    res=P.subs([(v**4,d_yyyy),(u*v**3,d_xyyy),(u**3*v,d_xxxy),(u**4,d_xxxx),(v**3,d_yyy),(u**3,d_xxx),\\\n",
    "                 (u**2*v**2,d_xxyy),(u*v**2,d_xyy),(u**2*v,d_xxy),(v**2,d_yy),(u*v,d_xy),(u**2,d_xx),(v,d_y),(u,d_x)])        \n",
    "    res=np.array(res).astype(np.float64)\n",
    "    return torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.5000,  0.0000,  0.5000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0\\\\0 & -0.5 & 0 & 0.5 & 0\\\\0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0,    0, 0,   0, 0],\n",
       "[0,    0, 0,   0, 0],\n",
       "[0, -0.5, 0, 0.5, 0],\n",
       "[0,    0, 0,   0, 0],\n",
       "[0,    0, 0,   0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试\n",
    "P=u\n",
    "print(subs_pdo(P))\n",
    "d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义旋转函数，这个函数将act on我们的partial differential operators\n",
    "def rotation(n):\n",
    "    I=[]\n",
    "    \n",
    "    for k in range(0,n):\n",
    "        \n",
    "        theta=(2*k*pi/n)\n",
    "        c, s = cos(theta), sin(theta)\n",
    "        a = c*u - s*v\n",
    "        b = s*u + c*v\n",
    "\n",
    "\n",
    "        basis=[d_0,subs_pdo(a),subs_pdo(b),subs_pdo(expand(a*a)),subs_pdo(expand(a*b)),subs_pdo(expand(b*b)),subs_pdo(expand(a*a*b)),\\\n",
    "               subs_pdo(expand(a*b*b)),subs_pdo(expand(a*a*b*b))]\n",
    "        basis=torch.stack(basis)\n",
    "        I.append(basis)\n",
    "        \n",
    "    return torch.stack(I)\n",
    "       \n",
    "       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义PDO_conv2D的Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_combination(pdo,beta,output_rot_num):\n",
    "    L=[]\n",
    "    for i in range(output_rot_num):\n",
    "        #beta.size:1*9, pdo[i].size: 9*5*5, kernel_size=1*5*5\n",
    "        term = beta[:, :,None, None] * pdo[i]\n",
    "        kernel=torch.sum(term,1)\n",
    "        L.append(kernel)#L_size: 8*1*5*5\n",
    "            \n",
    "    a=torch.stack(L,1)#a_size: 1*8*5*5\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "\n",
    "class PDO_Conv2D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, input_rot_num=1, output_rot_num=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.input_rot_num = input_rot_num\n",
    "        self.output_rot_num = output_rot_num\n",
    "        #定义beta\n",
    "        self.betas = nn.Parameter(torch.randn(self.in_channels*self.out_channels*self.input_rot_num,1,9))\n",
    "        #nn.ParameterList([nn.Parameter(torch.randn(1,9)) for i in range(out_channels*in_channels*input_rot_num)] )#betas: (56*7)*1*9\n",
    "        ##暂时未用kaiming_initialization\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "           \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.betas, a=math.sqrt(5),nonlinearity='relu')\n",
    "        \n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        pdo=rotation(self.output_rot_num) #pdo: 8*9*5*5\n",
    "        pdo=pdo.cuda()\n",
    "        \n",
    "        betas=self.betas.view(self.in_channels*self.input_rot_num,self.out_channels,1,9)\n",
    "        \n",
    "        U=[]\n",
    "        for i in range(self.in_channels*self.input_rot_num): \n",
    "            in_kernel=torch.cat([compute_linear_combination(pdo, beta, self.output_rot_num) for beta in betas[i]]) #betas[i]_size:7*1*9 in_kernel: 7,8,5,5\n",
    "            U.append(in_kernel)\n",
    "        real_kernel=torch.stack(U) #1*7*8*5*5 & 56*7*8*5*5\n",
    "\n",
    "             \n",
    "        shape = (self.out_channels * self.output_rot_num,\n",
    "                    self.in_channels * self.input_rot_num,    #56*1*5*5 & 56*56*5*5\n",
    "                    5, 5)\n",
    "        real_kernel = real_kernel.view(shape)\n",
    "        real_kernel=real_kernel.float()\n",
    "        \n",
    "\n",
    "        input_shape = input.size()\n",
    "        #input_size: 128,1,h,w & 128,56,h,w\n",
    "        input = input.view(input_shape[0], self.in_channels*self.input_rot_num, input_shape[-2], input_shape[-1])\n",
    "        #y_size: 128,56,h,w\n",
    "        y = F.conv2d(input, weight=real_kernel, bias=None, stride=1,\n",
    "                        padding=1)\n",
    "        \n",
    "        batch_size, _, ny_out, nx_out = y.size()\n",
    "        y = y.view(batch_size, self.out_channels*self.output_rot_num, ny_out, nx_out)\n",
    "        #y_size: 128,7,8,h,w\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P8_PDO_Conv_Z2(PDO_Conv2D):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(P8_PDO_Conv_Z2, self).__init__(in_channels=1, out_channels=7,input_rot_num=1, output_rot_num=8)\n",
    "\n",
    "\n",
    "class P8_PDO_Conv_P8(PDO_Conv2D):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(P8_PDO_Conv_P8, self).__init__(in_channels=7, out_channels=7,input_rot_num=8, output_rot_num=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义PDO_eConvs神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDO_eConvs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PDO_eConvs, self).__init__()\n",
    "        self.conv1 = P8_PDO_Conv_Z2(1, 7,1,8)\n",
    "        self.conv2 = P8_PDO_Conv_P8(7, 7,8,8)\n",
    "        self.conv3 = P8_PDO_Conv_P8(7, 7,8,8)\n",
    "        self.conv4 = P8_PDO_Conv_P8(7, 7,8,8)\n",
    "        self.conv5 = P8_PDO_Conv_P8(7, 7,8,8)\n",
    "        self.conv6 = P8_PDO_Conv_P8(7, 7,8,8)\n",
    "        self.dropout=nn.Dropout(p=0.2)\n",
    "        self.bn2 = nn.BatchNorm2d(56)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(4*4*7*8, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.bn2(F.relu(self.conv1(x))))\n",
    "        x = self.dropout(self.bn2(F.relu(self.conv2(x))))\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(self.bn2(F.relu(self.conv3(x))))\n",
    "        x = self.dropout(self.bn2(F.relu(self.conv4(x))))\n",
    "        x = self.dropout(self.bn2(F.relu(self.conv5(x))))\n",
    "        x = self.dropout(self.bn2(F.relu(self.conv6(x))))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        y=torch.nn.functional.log_softmax(x)\n",
    "\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxpool2=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "a=maxpool2(torch.randn(1,1,4,4))\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDO_eConvs(\n",
      "  (conv1): P8_PDO_Conv_Z2()\n",
      "  (conv2): P8_PDO_Conv_P8()\n",
      "  (conv3): P8_PDO_Conv_P8()\n",
      "  (conv4): P8_PDO_Conv_P8()\n",
      "  (conv5): P8_PDO_Conv_P8()\n",
      "  (conv6): P8_PDO_Conv_P8()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=896, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-659b75651d94>:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    }
   ],
   "source": [
    "net=PDO_eConvs()\n",
    "net.apply(init_weights)\n",
    "print(net)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: \n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    print(\"training on \", device)\n",
    "    loss = torch.nn.NLLLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y=y.type(torch.LongTensor)\n",
    "            y = y.to(device)\n",
    "            y=y.view(1,-1)[0]\n",
    "                   \n",
    "            y_hat = net(X)\n",
    "            \n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "        \n",
    "        if epoch==(0.5*num_epochs or 0.75*num_epochs):\n",
    "            lr=0.1*lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-5f4db8ede61c>:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y=torch.nn.functional.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.5968, train acc 0.118, test acc 14.501, time 935.0 sec\n"
     ]
    }
   ],
   "source": [
    "train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
